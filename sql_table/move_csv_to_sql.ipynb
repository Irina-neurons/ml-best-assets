{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud.sql.connector import Connector\n",
    "import sqlalchemy\n",
    "from utils_sql import *\n",
    "from sqlalchemy import text, Table, Column, String, Integer, Float, MetaData, PrimaryKeyConstraint\n",
    "import pyodbc\n",
    "\n",
    "img_benchmark = \"image_nvai_benchmarks\"\n",
    "img_metrics = \"image_nvai_metrics\"\n",
    "vid_benchmarks = \"video_nvai_benchmarks\"\n",
    "vid_metrics = \"video_nvai_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_PATH = \"/Users/irinakw/Library/CloudStorage/GoogleDrive-i.white@neuronsinc.com/Shared drives/HQ - R&D/Benchmark Documents/\"\n",
    "\n",
    "data_img_benchmarks = pd.read_csv(BENCHMARK_PATH + f'Master Sheets/Benchmark CSVs/insights_image_level_newmetrics.csv')\n",
    "data_img_metrics = pd.read_csv(BENCHMARK_PATH + 'eng_mem_images_total_metrics.csv')\n",
    "\n",
    "data_vid_benchmarks = pd.read_csv(BENCHMARK_PATH + f'Master Sheets/Benchmark CSVs/insights_video_level_newmetrics.csv')\n",
    "data_vid_metrics = pd.read_csv(BENCHMARK_PATH + 'eng_mem_videos_total_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: ['data_vid_benchmarks', 'image_nvai_benchmarks']\n"
     ]
    }
   ],
   "source": [
    "# Create SQLAlchemy engine\n",
    "engine = sqlalchemy.create_engine(\n",
    "    \"postgresql+pg8000://\",\n",
    "    creator=get_connection\n",
    ")\n",
    "inspector = sqlalchemy.inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(\"Tables in the database:\", tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Connection' object has no attribute 'raw_connection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m BENCHMARK_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Sheets/Benchmark CSVs/insights_video_level_newmetrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m----> 4\u001b[0m     raw_conn \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m()  \u001b[38;5;66;03m# Get raw DB-API connection\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m raw_conn\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;66;03m# Define primary key columns (based on your schema)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ml-best-assets/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1488\u001b[0m, in \u001b[0;36m_ConnectionFairy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m-> 1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdbapi_connection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Connection' object has no attribute 'raw_connection'"
     ]
    }
   ],
   "source": [
    "csv_file_path = BENCHMARK_PATH + f'Master Sheets/Benchmark CSVs/insights_video_level_newmetrics.csv'\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    raw_conn = conn.connection  # Access the underlying connection directly for pg8000\n",
    "    try:\n",
    "        with raw_conn.cursor() as cursor:\n",
    "            # Define primary key columns (based on your schema)\n",
    "            primary_key_columns = [\n",
    "                \"industry_category\", \"industry_subcategory\", \"usecase_category\",\n",
    "                \"usecase_subcategory\", \"platform\", \"device\", \"context\",\n",
    "                \"metric\", \"time\", \"type\"\n",
    "            ]\n",
    "            # Fetch existing rows from the database\n",
    "            query = f\"\"\"\n",
    "            SELECT {', '.join(primary_key_columns)}\n",
    "            FROM data_vid_benchmarks;\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            existing_keys = {tuple(row) for row in cursor.fetchall()}\n",
    "\n",
    "            # Load CSV into a DataFrame and filter missing rows\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            df[\"composite_key\"] = df[primary_key_columns].apply(tuple, axis=1)\n",
    "            df_missing = df[~df[\"composite_key\"].isin(existing_keys)]\n",
    "            df_missing = df_missing.drop(columns=[\"composite_key\"])  # Drop helper column\n",
    "            print(f\"Number of missing rows to add: {len(df_missing)}\")\n",
    "\n",
    "            if len(df_missing) > 0:\n",
    "                # Write missing rows to a temporary CSV file\n",
    "                temp_csv_path = \"missing_rows.csv\"\n",
    "                df_missing.to_csv(temp_csv_path, index=False)\n",
    "\n",
    "                # Use COPY command to upload missing rows\n",
    "                with open(temp_csv_path, \"r\") as f:\n",
    "                    cursor.copy_expert(\"\"\"\n",
    "                    COPY data_vid_benchmarks (industry_category, industry_subcategory, usecase_category,\n",
    "                                              usecase_subcategory, platform, device, context,\n",
    "                                              metric, time, type, num_points, lower, upper, delta)\n",
    "                    FROM STDIN\n",
    "                    WITH CSV HEADER DELIMITER ',';\n",
    "                    \"\"\", f)\n",
    "                raw_conn.commit()\n",
    "                print(\"Missing rows uploaded successfully!\")\n",
    "\n",
    "                # Clean up the temporary file\n",
    "                import os\n",
    "                os.remove(temp_csv_path)\n",
    "            else:\n",
    "                print(\"No new rows to add.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        raw_conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData()\n",
    "column_types = infer_sqlalchemy_types(data_vid_benchmarks)\n",
    "vid_benchmarks = Table(\n",
    "    \"data_vid_benchmarks\",\n",
    "    metadata,\n",
    "    *[\n",
    "        Column(col, col_type) for col, col_type in column_types.items()\n",
    "    ],\n",
    "    PrimaryKeyConstraint(\n",
    "        \"industry_category\", \"industry_subcategory\", \"usecase_category\",\n",
    "        \"usecase_subcategory\", \"platform\", \"device\", \"context\",\n",
    "        \"metric\", \"time\", \"type\"\n",
    "    )\n",
    ")\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"DRIVER\": \"PostgreSQL Unicode\",\n",
    "    \"SERVER\": \"127.0.0.1\",\n",
    "    \"PORT\": \"5432\",\n",
    "    \"DATABASE\": \"assets-experiment\",\n",
    "    \"UID\": \"i.white@neuronsinc.com\",\n",
    "    \"PWD\": \"\",\n",
    "}\n",
    "\n",
    "# Create a connection and a cursor\n",
    "conn_string = \";\".join([f\"{key}={value}\" for key, value in DB_CONFIG.items()])\n",
    "print(conn_string)\n",
    "conn = pyodbc.connect(conn_string)\n",
    "cursor = conn.cursor()\n",
    "print(\"Connection successful!\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT table_name\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'public';\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "for _ in tables:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Use `text` to create an executable SQL statement\n",
    "    query = text(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'public';\n",
    "    \"\"\")\n",
    "    result = conn.execute(query)\n",
    "    print(\"Tables in the database:\")\n",
    "    for row in result:\n",
    "        print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_table(engine, table_name):\n",
    "    query = text(f\"\"\"\n",
    "        SELECT COUNT(*) AS row_count\n",
    "        FROM {table_name}\n",
    "    \"\"\")  # Query to count rows in the table\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query)\n",
    "        row_count = result.scalar()  # Fetch the scalar value (row count)\n",
    "        print(f\"Number of all rows in the table '{table_name}': {row_count}\")\n",
    "\n",
    "print(f\"Number of rows in the file img_benchmarks: {data_img_benchmarks.shape[0]}\")\n",
    "count_rows_in_table(engine, img_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table(df, table_name):    \n",
    "    print(f\"Number of rows in the CSV file: {len(df)}\")\n",
    "    \n",
    "    # Filter missing rows\n",
    "    comparison_columns = df.columns.tolist()\n",
    "    query = f\"SELECT {', '.join(comparison_columns)} FROM {table_name}\"\n",
    "    cursor.execute(query)\n",
    "    existing_rows = {tuple(row) for row in cursor.fetchall()}  \n",
    "    df[\"composite_key\"] = df[comparison_columns].apply(tuple, axis=1)\n",
    "    df_missing = df[~df[\"composite_key\"].isin(existing_rows)]\n",
    "    df_missing = df_missing.drop(columns=[\"composite_key\"])  # Drop helper column\n",
    "    print(f\"Number of missing rows to add: {len(df_missing)}\")\n",
    "\n",
    "    if len(df_missing) > 0:\n",
    "        # Bulk insert missing rows into the table\n",
    "        for index, row in df_missing.iterrows():\n",
    "            row_values = tuple(row)  # Convert row to a tuple\n",
    "            placeholders = \", \".join([\"?\"] * len(row))  # Prepare placeholders\n",
    "            query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "            cursor.execute(query, row_values)\n",
    "        conn.commit()\n",
    "    else:\n",
    "        print(\"No new rows to add.\")\n",
    "\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_table(data_img_benchmarks, img_benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_INDUSTRY = 'all'\n",
    "DESIRED_SUBINDUSTRY = 'all'\n",
    "DESIRED_USECASE = 'digital_ads'\n",
    "DESIRED_SUBUSECASE = 'out_of_home_ads'\n",
    "DESIRED_PLATFORM = []  \n",
    "\n",
    "query_img_benchmarks(\n",
    "    engine, usecase_category=DESIRED_USECASE\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
